---
title: "ST558 Project 2"
author: "Aries Zhou & Jiatao Wang"
date: "10/30/2021"
params: 
  channel: "data_channel_is_lifestyle"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, fig.align='center', fig.path = "./")
```

## Introduction  


This is a R project using the exploratory data analysis and supervised statistical learning method to analyze a data set.  
This data set is called __Online News Popularity Data Set__ and you can access the data set [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)    

The following variables are included in this data.  
 
0. `url`: URL of the article (non-predictive)   
1. `timedelta`: Days between the article publication and the dataset acquisition (non-predictive)   
2. `n_tokens_title`: Number of words in the title   
3. `n_tokens_content`: Number of words in the content   
4. `n_unique_tokens`: Rate of unique words in the content   
5. `n_non_stop_words`: Rate of non-stop words in the content   
6. `n_non_stop_unique_tokens`: Rate of unique non-stop words in the content   
7. `num_hrefs`: Number of links   
8. `num_self_hrefs`: Number of links to other articles published by Mashable   
9. `num_imgs`: Number of images   
10. `num_videos`: Number of videos   
11. `average_token_length`: Average length of the words in the content   
12. `num_keywords`: Number of keywords in the metadata   
13. `data_channel_is_lifestyle`: Is data channel 'Lifestyle'?   
14. `data_channel_is_entertainment`: Is data channel 'Entertainment'?   
15. `data_channel_is_bus`: Is data channel 'Business'?   
16. `data_channel_is_socmed`: Is data channel 'Social Media'?   
17. `data_channel_is_tech`: Is data channel 'Tech'?   
18. `data_channel_is_world`: Is data channel 'World'?   
19. `kw_min_min`: Worst keyword (min. shares)   
20. `kw_max_min`: Worst keyword (max. shares)   
21. `kw_avg_min`: Worst keyword (avg. shares)   
22. `kw_min_max`: Best keyword (min. shares)   
23. `kw_max_max`: Best keyword (max. shares)   
24. `kw_avg_max`: Best keyword (avg. shares)   
25. `kw_min_avg`: Avg. keyword (min. shares)   
26. `kw_max_avg`: Avg. keyword (max. shares)   
27. `kw_avg_avg`: Avg. keyword (avg. shares)   
28. `self_reference_min_shares`: Min. shares of referenced articles in Mashable   
29. `self_reference_max_shares`: Max. shares of referenced articles in Mashable   
30. `self_reference_avg_sharess`: Avg. shares of referenced articles in Mashable   
31. `weekday_is_monday`: Was the article published on a Monday?   
32. `weekday_is_tuesday`: Was the article published on a Tuesday?   
33. `weekday_is_wednesday`: Was the article published on a Wednesday?   
34. `weekday_is_thursday`: Was the article published on a Thursday?   
35. `weekday_is_friday`: Was the article published on a Friday?   
36. `weekday_is_saturday`: Was the article published on a Saturday?   
37. `weekday_is_sunday`: Was the article published on a Sunday?   
38. `is_weekend`: Was the article published on the weekend?   
39. `LDA_00`: Closeness to LDA topic 0   
40. `LDA_01`: Closeness to LDA topic 1   
41. `LDA_02`: Closeness to LDA topic 2   
42. `LDA_03`: Closeness to LDA topic 3   
43. `LDA_04`: Closeness to LDA topic 4   
44. `global_subjectivity`: Text subjectivity   
45. `global_sentiment_polarity`: Text sentiment polarity   
46. `global_rate_positive_words`: Rate of positive words in the content   
47. `global_rate_negative_words`: Rate of negative words in the content   
48. `rate_positive_words`: Rate of positive words among non-neutral tokens   
49. `rate_negative_words`: Rate of negative words among non-neutral tokens   
50. `avg_positive_polarity`: Avg. polarity of positive words   
51. `min_positive_polarity`: Min. polarity of positive words   
52. `max_positive_polarity`: Max. polarity of positive words   
53. `avg_negative_polarity`: Avg. polarity of negative words   
54. `min_negative_polarity`: Min. polarity of negative words   
55. `max_negative_polarity`: Max. polarity of negative words   
56. `title_subjectivity`: Title subjectivity   
57. `title_sentiment_polarity`: Title polarity   
58. `abs_title_subjectivity`: Absolute subjectivity level   
59. `abs_title_sentiment_polarity`: Absolute polarity level   
60. `shares`: Number of shares (the response variables)   

There are lots of measurements/heterogeneous features of articles, including type of the data channel, number of images, number of videos, number of links, counts of words in the title/content, when it is published, summary statistics of polarity of positive/negative words and etc...  

The __main goal__ of this project is to use those features/explanatory variables to predict the popularity(number of the shares in social networks)  
Before conducting any method to fit the data with models, we want to do some exploratory data analysis (including some summary statistics and graphs) to visualize the data. And then, we will fit the data under regression setting.  
Supervised learning methods that will be used in this project include: linear regression, generalized linear model, lasso regression, random forest regression, boosted method, or any other method that we will find that could be applicable through our discovering of the data.  


### List of packages used:  

```{r packages}
library(dplyr)
library(tidyr)
library(ggcorrplot)
library(vcd)
library(caret)
library(class)
library(randomForest)
library(gbm)
library(readr)
library(leaps)
library(Matrix)
library(glmnet)
library(rmarkdown)
library(doParallel)
```

## Data Cleaning 

### Data  
Read in data and transpose data_channel_is* and weekday_is* columns into categorical columns.  

```{r transpose}
# import data
pop <- read_csv("OnlineNewsPopularity.csv")

# check if there is any missing or NA values in the data set 
anyNA(pop) # returned FALSE, so no missing values 

# convert the wide to long format (categorize data channel, and make them into one column)
new <- pop %>% pivot_longer(cols = data_channel_is_lifestyle:data_channel_is_world, names_to = "channel",values_to = 'logi.num.d') 
new_data <- new %>% filter(logi.num.d != 0) %>% select(-logi.num.d) # drop logical number

# merge those weekday columns into one.
Z <- new_data %>% pivot_longer(cols = weekday_is_monday:weekday_is_sunday, names_to = "weekday",values_to = 'logi.num.w') 
X <- Z %>% filter(logi.num.w != 0) %>% select(-logi.num.w) # drop logical numbers
```

Subset data on data channel of interest for analysis and set the params to do automation.  

```{r subset}
pop.data <- X %>% filter(channel == params$channel) %>% select(-1:-2)
pop.data$is_weekend <- as.factor(pop.data$is_weekend)
pop.data$weekday <- as.factor(pop.data$weekday)
```

Check if there are observations that are not marked by channel variable.
```{r}
nrow(new_data)< nrow(pop)
```
Since `nrow(new_data)< nrow(pop)` returned `TRUE`, indicating that there are some observations that are not in the types of channel listed in the data set.


## Exploratory Data Analysis   

### Summarizations And Graphs  

```{r str}
#check data structures.
str(pop.data)
```

```{r summary}
#summary stats for the response variable. 
summary(pop.data$shares)
```

The distribution of the response variable (shares) is:  

  -  **Right-skewed** if its mean is **greater** than its median.    
  -  **Left-skewed** if its mean is **less** than its median.    
  -  **Normal** if its mean **equals** to its median.    

Check correlations.    

```{r cor}
#get all numeric variables without collinearity
pop.data.num <- select(pop.data, is.numeric) %>% mutate_all(~(scale(.) %>% as.vector)) 
```

```{r lm.num, results='hide'}
# due to the large number of variables, try to get a best subset with the stepwise method. 
lm <- step(lm(shares ~ ., data = pop.data.num))
```

This is a multiple linear model selected based on AIC using step function.  
```{r}
lm$call[["formula"]] # 
```

plot of correlations.  
For this correlation plot, the color is red if two variables are positively correlated and is blue if two variables are negatively correlated.  
```{r cor.plot}
# get the selected subset
num.s <- pop.data.num %>% select(n_tokens_content , n_non_stop_words , n_non_stop_unique_tokens , num_hrefs , num_videos , kw_avg_max , kw_min_avg , kw_max_avg , kw_avg_avg , self_reference_min_shares , self_reference_avg_sharess , abs_title_subjectivity, shares)

# check correlations
cor <- round(cor(num.s, use="complete.obs"), 2)

# select correlated variables without variables with collinearity (ex. kw_min_min and kw_max_min to kw_avg_min)
plot.s <- num.s %>% select(n_tokens_content, n_non_stop_unique_tokens, n_non_stop_words, num_hrefs, num_videos, kw_avg_avg, shares)

# plot a correlation plot
cor.plot <- round(cor(plot.s, use="complete.obs"), 2)
ggcorrplot(cor.plot, hc.order = TRUE, type = "lower", lab = TRUE)
```

   


Some tables for selected data channel of interest showing the counts and percentage grouped by channel and weekday    
```{r tables}
#simple table displaying counts for different type of channel (all obs)
table(X$channel) 

#some summary stats grouped by channel and weekday
C1 <- X %>% 
    group_by( channel,weekday) %>% 
    summarise( percent = 100 * n() / nrow(X),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))
```

show percentage of channel and weekday grouped for all the observations. as well as means within each group  
```{r}
knitr::kable(C1)
```

Show percentage of each grouped category as well as stats  
```{r}
# using the subset data set containing weekday info in one column. 
table(pop.data$weekday)
C2 <- pop.data %>% 
    group_by( weekday ) %>% 
    summarise( percent = 100 * n() / nrow( pop.data ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))
knitr::kable(C2)

table(pop.data$channel, pop.data$is_weekend)

C3 <- pop.data %>% group_by(is_weekend)%>% 
    summarise( percent = 100 * n() / nrow( pop.data ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))
knitr::kable(C3)

```

### Graphical summaries  
```{r scatter1}
#Scatter plot for n_tokens_content v.s. Shares.
scatter.tc <- ggplot(data = pop.data, aes(x = n_tokens_content, y = shares))
scatter.tc + geom_point(aes(color = is_weekend)) + 
             geom_smooth(method = "lm") + 
             labs(title = "Number of Tokens in Content v.s. Shares", x = "Number of tokens in content", y = "shares") + 
             scale_color_discrete(name = "is_weekend")

```

If the linear regression line shows an upward trend, then articles with more words in content tend to be shared more often; if it shows a downward trend, then articles with more words in content tend to be shared less often.    

```{r scatter2}
#Scatter plot for Videos v.s. Shares.
scatter.video <- ggplot(pop.data,aes(x = num_videos, y =shares))
scatter.video + geom_point(aes(shape = is_weekend, color = weekday), size = 2) + 
                geom_smooth(method = "lm") + 
                labs(x = "Videos", y = "Shares", title = "Videos vs Shares ") +  
                scale_shape_manual(values = c(3:4))+
                scale_color_discrete(name = "weekday")+
                scale_shape_discrete(name="is_weekend")
```

Similarly, if the linear regression line shows an upward trend, then articles with more videos tend to be shared more often; if it shows a downward trend, then articles with more videos tend to be shared less often.    

```{r scatter3}
#Scatter plot for Number of Tokens in Content v.s. Number of Links.
scatter.stop <- ggplot(data = pop.data, aes(x = n_tokens_content, 
                                            y = num_hrefs))
scatter.stop + geom_point(aes(color = is_weekend)) + 
               geom_smooth(method = "lm") + 
               labs(title = "Number of Tokens in Content v.s. Number of Links", 
                    x = "Number of Tokens in Content", 
                    y = "Number of Links") + 
               scale_color_discrete(name = "is_weekend")

```

Observing from the plot, if the linear regression line is upward, there is a positive correlation relationship between the number of words and number of links in articles. The number of links increases as the number of words increases in the articles. If the linear regression line is downward, the result is the reverse.   


#### General plots   

This is a bar plot channel by weekend(is or not)   
 
```{r barplot1}
#bar plot channel by weekend
bar.weekend <-ggplot(X,aes(x = channel))
bar.weekend + geom_bar(aes(fill = as.factor(is_weekend)), 
                       position = "dodge") + 
              labs(x = "channel", y = "Count", 
                   title = "Channel by Weekend") +
              theme(axis.text.x = element_text(angle = 45, 
                                               vjust = 1, hjust = 1)) +
              scale_fill_discrete(name = "is_weekend") 
```
Observe the number of counts from the y-axis of this barplot, we can compare whether there is more shares on the weekends or less shares on weekends for each channels.  

Boxplot for different channels.      

```{r boxplot}  
box <- ggplot(X, aes(x = channel, y = shares))
box + geom_boxplot(position = "dodge") + 
      labs(x = "y", 
           title = "Boxplot for popularity with channel type ") + 
      scale_x_discrete(name = "channel") + 
      geom_jitter(aes(color = as.factor(weekday))) + 
      scale_y_continuous() + 
      theme(axis.text.x = element_text(angle = 45, 
                                       vjust = 1, hjust = 1)) +
      scale_color_discrete(name = "weekday")
```  
There are some outliers of shares if the points are away from the box.  
  
This is the bar plot : channel by weekday(stacked bar).   

```{r barplot2}  
s.bar <- ggplot(X, aes(x = weekday))
s.bar + geom_bar(aes(fill = as.factor(channel)), 
                 position = "stack",
                 show.legend = NA) +         labs(x = "weekday") + 
        scale_fill_discrete(name = "channel") + 
        theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
        labs(title = "weekday by channel ")
 # or  
g <- ggplot(X, aes(x = channel))
g + geom_bar(aes(fill = as.factor(weekday)),
             position = "stack",show.legend = NA) + 
    labs(x = "channel")+ 
    scale_fill_discrete(name = "weekday") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    labs(title = " channel by weekday ")
```

The stacked bar helps to understand the proportions between each channels/weekdays by comparing the size of rectangles. (Different colors represent different channels/weekdays.)   

## Modeling(Regression Settings)

Using parallel computing to speed up computations
```{r para.comp}
 
cores <- detectCores()
cl <- makePSOCKcluster(cores-1)
registerDoParallel(cl)
```

Using 5-fold Cross-Validation.  
```{r cv}
ctrl <- trainControl(method = "cv", number = 5)
```
 
### Split the data set.   

Before fitting any predictive models, we tried some methods that could help reduce the dimension of data.  
We randomly selected some predictors of interest and perform the best subset selection under the condition of least square linear regression.    

```{r lm.best.sub}
# for the variable that can be used in the linear regression model. 
# try best subset selection, select number of variables using adjusted R^2, and mallow's cp, BIC,
 
final <- pop.data %>% select(n_tokens_content , n_non_stop_words , n_non_stop_unique_tokens , num_hrefs , num_imgs,num_keywords, num_videos , kw_avg_max , kw_min_avg , kw_max_avg , kw_avg_avg , self_reference_min_shares , self_reference_avg_sharess ,global_rate_positive_words,rate_positive_words, abs_title_subjectivity,abs_title_sentiment_polarity,shares)

set.seed(1033)

# split the subset data into training and testing set. Use p = 0.7.
train.index.sub <- createDataPartition(y = final$shares, p = 0.7, list = F)
train.sub <- final[train.index.sub, ] # training set
test.sub <- final[-train.index.sub, ] # test set

regression1 <- regsubsets(shares ~., data = train.sub, nvmax=17)
hh1<-summary(regression1)

# this is the indicators of the variables that are supposed to be included in the model each time(iteration)
knitr::kable(hh1$which)
```

A simple function that helps to get the model for the best subset selection.  

```{r model.bs}
get_model_formula <- function(id, object, outcome){
  # get models data
  models <- hh1$which[id,-1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  # Build model formula
  as.formula(paste0(outcome, "~", predictors))
}
```

Using mallow's cp, BIC and Adjusted R^2, to do model selection.   

```{r select.bs}
gk <- data.frame(
  Adj.R2 = which.max(hh1$adjr2),
  CP = which.min(hh1$cp),
  BIC = which.min(hh1$bic)
)
knitr::kable(gk)

par(mfrow=c(2,2))
plot(hh1$cp ,xlab="Number of Variables ",ylab="Mallow's Cp", type='b')
plot(hh1$adjr2 ,xlab="Number of Variables ",ylab="Adjusted R^2 ", type='b')
plot(hh1$bic,xlab="Number of Variables ",ylab="BIC ", type='b')

# using the mallow's cp to choose model size. 
best_subset_model <- get_model_formula(which.min(hh1$cp),models,"shares")
```

After using the best subset selection, some important variables are: `n_tokens_content`, `num_videos`,   `n_non_stop_words`, `n_non_stop_unique_tokens`, `self_reference_min_shares`, `kw_avg_avg`, `abs_title_subjectivity`, and `kw_max_avg`.   

### Linear Regression  

linear regression models are models that shows linear relationship between the response variable (y) and the predictor variable(s) (x). The coefficients of each predictor variable in the model demonstrate how increase/decrease of the predictor variables influences the change of the response variable.  

Since using all predictors is time-consuming and hard to render in automation, we use the random selected variables(p = 17) from the best subset selection to fit the linear regression models.  

```{r lm, results='hide'}
# check the model selected by the best subset selection 
lm.fit1 <- train(best_subset_model, data = train.sub,
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)

# Consider all variables in the best subset, pick a model using forward selection method.
lm2 <- step(lm(shares ~ . , data = train.sub), direction = "forward")

# fit the model chosen from the forward selection for all linear terms
lm.fit2 <- train(lm2$call[["formula"]], data = train.sub, 
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)

# Consider variables selected with forward selection (with 2-way interactions).
lm3 <- step(lm(lm2$call[["formula"]], data = train.sub), scope = . ~.^2, direction = "both", use.start = TRUE)

# fit the model chosen from both forward and backward method for the interaction terms and linear terms. 
lm.fit3 <- train(lm3$call[["formula"]], data = train.sub, 
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)
```

```{r fit3}
# create a table to compare the results of linear regression from training data 
lm.compare <- data.frame(models= c("lm.fit1", "lm.fit2","lm.fit3"), 
                         results = bind_rows(lm.fit1$results[2:4], lm.fit2$results[2:4], lm.fit3$results[2:4]))
knitr::kable(lm.compare) 
```

Check Linear Regression model performance on test set   

```{r lm.pred}
# Best subset
pred.lm1 <- predict(lm.fit1, newdata = test.sub)
test.RMSE.lm1 <- RMSE(pred.lm1, test.sub$shares)

# Forward
pred.lm2 <- predict(lm.fit2, newdata = test.sub)
test.RMSE.lm2 <- RMSE(pred.lm2, test.sub$shares)

# both
pred.lm3 <- predict(lm.fit3, newdata = test.sub)
test.RMSE.lm3 <- RMSE(pred.lm3, test.sub$shares)
```


### Lasso Regression   
Since lasso perform the variable selection, we tried to use Lasso Regression(adding tuning parameter/ penalty)   
Lasso using all the predictors and get the test MSE   

```{r lasso}
pop.data <- X %>% filter(channel == params$channel) %>% select(-1:-2)

# split data
train.index <- createDataPartition(y = pop.data$shares, p = 0.7, list = F)

train.lasso <- pop.data[train.index, ] # training set
test.lasso <- pop.data[-train.index, ] # test set

# using all predictors (52 predictors)
cv.out.full <- cv.glmnet(as.matrix(train.lasso[,-47:-48]), train.lasso$shares, alpha=1)

#MSE versus the log(lambda)
plot(cv.out.full,main = "tuning parameter selection for lasso(full predictors)")
best.lambda.full <- cv.out.full$lambda.min

#fitting the lasso regression 
lasso.fit.full <- glmnet(train.lasso[,-46:-48] ,train.lasso$shares, alpha = 1, lambda = best.lambda.full)
lasso.coef.full <- predict(lasso.fit.full, type = "coefficients")
print(lasso.coef.full)
```

Lasso method using the 17 predictors.    
```{r lasso.18}
#using selected predictors (17 predictors) 
#use k-fold cv to select best lambda for the lasso regression 
cv.out <- cv.glmnet(as.matrix(train.sub), train.sub$shares, alpha=1)
#MSE versus the log(lambda)
plot(cv.out,main = "tuning parameter selection for lasso(17 predictors)")
best.lambda <- cv.out$lambda.min

#fitting the lasso regression 
lasso.fit.18 <- glmnet(train.sub[,-18] ,train.sub$shares, alpha = 1, lambda = best.lambda)
lasso.coef <- predict(lasso.fit.18, type = "coefficients")
print(lasso.coef)
```

Check Lasso performance on test set.    

```{r lasso.pred}
# Using the 17 predictors
lasso.partial.pred <- predict(lasso.fit.18, newx= as.matrix(test.sub[,-18]))
test.RMSE.lasso.partial <- RMSE(lasso.partial.pred, test.sub$shares)

# Using all predictors (52 predictors)
lasso.pred.full <- predict(lasso.fit.full, newx= as.matrix(test.lasso[,-46:-48]))
test.RMSE.lasso.full <- RMSE(lasso.pred.full, test.lasso$shares)
```

### Random Forest Regression   

Random Forest is an ensemble tree-based model that helps to solve regression and classification problems. Its algorithm consists of many decision trees, which its forest is generated and train through bootstrapping, expanding the forest improves its precision.   

Random Forest regression is used to de-correlate each model fitting. We use the model the previously get from the forward and both selection method to fit the random forest regression. Take computation limit, and time consumption into account, there are only a few tuning parameters set for the test. 

```{r rf}
# create data frame for tuning parameter
rf.tGrid <- expand.grid(mtry = seq(from = 2, to = 7, by = 1))

# train the Random Forest model
# use model selected by forward selection
rf.fit1 <- train(lm2$call[["formula"]], data = train.sub, 
             method = "rf", trControl = ctrl, 
             preProcess = c("center", "scale"), 
             tuneGrid = rf.tGrid)

# use model selected by both selection
rf.fit2 <- train(lm3$call[["formula"]], data = train.sub, 
             method = "rf", trControl = ctrl, 
             preProcess = c("center", "scale"), 
             tuneGrid = rf.tGrid)

# plot RMSE for each iteration
plot(rf.fit1$results$mtry, rf.fit1$results$RMSE, 
     xlab = "mtry",ylab = "RMSE",type = 'p',main = 'random forest')
plot(rf.fit2$results$mtry, rf.fit2$results$RMSE, 
     xlab = "mtry",ylab = "RMSE",type = 'p',main = 'random forest')
```

Check Random Forest model performance on test set.    

```{r rf.pred}
# start model selected by forward
pred.rf1 <- predict(rf.fit1, newdata = test.sub)
test.RMSE.rf1 <- RMSE(pred.rf1, test.sub$shares)

# start model selected by both
pred.rf2 <- predict(rf.fit2, newdata = test.sub)
test.RMSE.rf2 <- RMSE(pred.rf2, test.sub$shares)
```



### Boosting model. (Stochastic Gradient Boosting)     

Boosting is a slow learn method that learn from the previous fit each time in order to prevent over fitting. We also use the model the previously get from the forward and both selection method to fit the boosted.  
Boosting tree have several tuning parameters, also, due to some limitation, the number of tuning parameters and cross validation number is set to be small.  

```{r boost,results='hide'}
# set tuning parameters
tune1 = c(25,50,100,150,200)
tune2 = c(1:10)
tune3 = 0.01
tune4 = 10
boos.grid <- expand.grid(n.trees = tune1, 
                         interaction.depth = tune2, 
                         shrinkage = tune3, 
                         n.minobsinnode = tune4)

# train the Boosted Tree model
# use model selected by forward selection
boostTreefit1 <- train(lm2$call[["formula"]], data = train.sub, 
                method = "gbm",
                preProcess = c("center","scale"),
                trControl = ctrl,
                tuneGrid = boos.grid)

# use model selected by both selection
boostTreefit2 <- train(lm3$call[["formula"]], data = train.sub, 
                 method = "gbm",
                 preProcess = c("center","scale"),
                 trControl = ctrl,
                 tuneGrid = boos.grid)

```

Plot for the RMSE associated with tuning parameters  
```{r}
par(mfrow=c(2,2))
plot(boostTreefit1$results$n.trees, boostTreefit1$results$RMSE, 
     xlab = "n.trees",ylab = "RMSE", type = 'p',main = 'boosted 1')
plot(boostTreefit1$results$interaction.depth, boostTreefit1$results$RMSE, 
     xlab = "subtrees",ylab = "RMSE",type = 'p',main = 'boosted 1')
plot(boostTreefit1$results$interaction.depth, boostTreefit1$results$Rsquared, 
     xlab = "subtrees",ylab = "R^2",type = 'p',main = 'boosted 1')

par(mfrow=c(2,2))
plot(boostTreefit2$results$n.trees, boostTreefit2$results$RMSE, 
     xlab = "n.trees",ylab = "RMSE", type = 'p',main = 'boosted 2')
plot(boostTreefit2$results$interaction.depth, boostTreefit2$results$RMSE, 
     xlab = "subtrees",ylab = "RMSE",type = 'p',main = 'boosted 2')
plot(boostTreefit2$results$interaction.depth, boostTreefit2$results$Rsquared, 
     xlab = "subtrees",ylab = "R^2",type = 'p',main = 'boosted 2')
```

Check Boosted Tree model performance on test set.   

```{r boost.pred}
# start model selected by forward
pred.boost1 <- predict(boostTreefit1 , newdata = test.sub)
test.RMSE.boost1 <- RMSE(pred.boost1, test.sub$shares)

# start model selected by both
pred.boost2 <- predict(boostTreefit2 , newdata = test.sub)
test.RMSE.boost2 <- RMSE(pred.boost2, test.sub$shares)

```

```{r}
# done with parallel computing 
stopCluster(cl)
```

### Discussion and Model Selection     

 - lm.fit1 is chosen by the best subset selection   
 - lm.fit2 is using forward selection to select variables of most interest.    
 - lm.fit3 is adding the interaction terms to the model fitting  
 - lasso.fit.full is using the lasso regression to fit the model for all predictors (it also perform variable selection)    
 - lasso.fit.18 is using the lasso regression to fit the model for random selected predictors(17 predictors )    
 - rf.fit1/rf.fit2 is using de-correlated method to reduce the variance  
 - boost.fit is using cross validation to select appropriate tuning parameter for the boosted model and use it for prediction.    

This is a simple table containing these methods and the Root Mean Square Error for each model fitting.  
```{r}
all.compare <- data.frame(models= c("lm.fit1", "lm.fit2","lm.fit3",
                                    "lasso.fit.full","lasso.fit.18",
                                    "rf.fit1","rf.fit2",
                                    "boostTreefit1","boostTreefit2"), 
                          test_RMSE = c(test.RMSE.lm1, test.RMSE.lm2, test.RMSE.lm3,
                                            test.RMSE.lasso.full, test.RMSE.lasso.partial,
                                            test.RMSE.rf1, test.RMSE.rf2, 
                                            test.RMSE.boost1, test.RMSE.boost2))
knitr::kable(all.compare) 
```

Select model with lowest RMSE.    

```{r}
select <- all.compare %>% filter(test_RMSE == min(test_RMSE))
select
```
The model with lowest RMSE is the `r select[[1]]` model. This model has the best performance on test set. 

### Automation of data channels  

We need to read in libraries as well as some data set before knitting the automation part.  

```{r, eval = FALSE}
channels <- unique(X$channel)
output_file <- paste0(channels,".md")

params = lapply(channels, FUN = function(x){list(channel = x)})

reports <- tibble(output_file, params)

library(rmarkdown)

apply(reports, MARGIN = 1,
      FUN = function(x){
        render(input = "./ST558_Project_2.Rmd",
               output_format = "github_document", 
               output_file = x[[1]], 
               params = x[[2]])
      })
```