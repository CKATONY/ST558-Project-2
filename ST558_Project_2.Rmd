---
title: "ST558 Project 2"
author: "Aries Zhou"
date: "10/19/2021"
---

```{r, echo = FALSE, eval = FALSE}
rmarkdown::render("./ST558_Proj_1.Rmd", 
                  output_format = "github_document", 
                  output_dir = "./",
                  output_options = list(html_preview = FALSE, keep_html=FALSE))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.align='center', fig.path = "./")
```

## Introduction  

### Description the data for this project 
This is a R project using the exploratory data analysis and supervised statistical learning method to analyze a data set.  
This data set is called __Online News Popularity Data Set__ and you can access the data set [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)  
There are lots of measurements/heterogeneous features of articles, including type of the data channel, number of images, number of videos, number of links, counts of words in the title/content, when it is published, summary statistics of polarity of positive/negative words and etc...  
The __main goal__ of this project is to use those features/explanatory variables to predict the popularity(number of the shares in social networks)  
Before conducting any method to fit the data with models, we want to do some exploratory data analysis (including some summary statistics and graphs) to visualize the data. And then, we will fit the data under regression setting as well as classification setting.  
Supervised learning methods that will be used in this project include: linear regression, generalized linear model, logistic regression, random forest regression, random forest classification, boosted tree method, or any other method that we will find applicable through our discovering of the data.  


List of packages used:  
```{r}
library(dplyr)
library(tidyr)
library(ggcorrplot)
library(vcd)
library(caret)
library(class)
library(randomForest)
library(gbm)
library(readr)
```

### Data  

```{r}
# import data
pop <- read_csv("OnlineNewsPopularity.csv")

anyNA(pop)

#convert the wide to long format (categorize data channel, and make them into one column)
new <- pop %>% pivot_longer(cols = data_channel_is_lifestyle:data_channel_is_world, names_to = "channel",values_to = 'things') 
new_data <- new %>% filter(things != 0) %>% select(-things) 
pop.data2 <- new_data %>% subset(channel == 'data_channel_is_lifestyle') %>% select( -1:-2)

#there are some observations that are not in the types of channel listed in the data set. 
nrow(new_data)< nrow(pop)



# subset data on data channel of interest
pop.data <- pop %>% subset(data_channel_is_lifestyle == 1) %>% select(-starts_with("data_channel_is_"), -1:-2)

str(pop.data) 
```





### Summarizations  

Some attempts   

```{r}
pop.data$weekday_is_monday <- as.factor(pop.data$weekday_is_monday)
pop.data$weekday_is_tuesday <- as.factor(pop.data$weekday_is_tuesday)
pop.data$weekday_is_wednesday <- as.factor(pop.data$weekday_is_wednesday)
pop.data$weekday_is_thursday <- as.factor(pop.data$weekday_is_thursday)
pop.data$weekday_is_friday <- as.factor(pop.data$weekday_is_friday)
pop.data$weekday_is_saturday <- as.factor(pop.data$weekday_is_saturday)
pop.data$weekday_is_sunday <- as.factor(pop.data$weekday_is_sunday)
pop.data$is_weekend <- as.factor(pop.data$is_weekend)

summary(pop.data$shares)
# or we can  merge those column in to one categorical variable. 




# check the counts of binary variables.
pop.data.char <- pop.data %>% select(is.factor)
str(pop.data.char)

table(pop.data.char)

pop.data.num <- select(pop.data, is.numeric) %>% mutate_all(~(scale(.) %>% as.vector)) 

# check correlations
cor <- round(cor(pop.data.num, use="complete.obs"), 2)

# Consider all numeric variables.
lm <- step(lm(shares ~ . , data = pop.data.num), direction = "backward")

l.fit<- lm(lm$call[["formula"]], data = pop.data.num)
summary(l.fit)

lm$call[["formula"]]
gsub("[+]", ",", lm$call[["formula"]])

pop.data.num.s <- pop.data.num %>% select(n_tokens_content , n_non_stop_words , n_non_stop_unique_tokens , num_hrefs , num_videos , kw_avg_max , kw_min_avg , kw_max_avg , kw_avg_avg , self_reference_min_shares , self_reference_avg_sharess , abs_title_subjectivity)

cor.selected <- round(cor(pop.data.num, use="complete.obs"), 2)


ggcorrplot(cor.selected, hc.order = TRUE, type = "lower", lab = TRUE)




# for the variable that can be used in the linear regression model. 
# try best subset selection, adjusted R^2, and AIC, BIC,
install.packages("leaps")
library(leaps)
 
final <- pop.data %>% select(n_tokens_content , n_non_stop_words , n_non_stop_unique_tokens , num_hrefs , num_videos , kw_avg_max , kw_min_avg , kw_max_avg , kw_avg_avg , self_reference_min_shares , self_reference_avg_sharess , abs_title_subjectivity,shares)


train.index <- createDataPartition(y = final$shares, p = 0.7, list = F)
train.sub <- final[train.index, ] # training set
test.sub <- final[-train.index, ] # test set


regression1 <- regsubsets(shares ~., data = train.sub,nvmax=12)
hh1<-summary(regression1)


par(mfrow=c(2,2))
which.min(hh1$cp)
plot(hh1$cp ,xlab="Number of Variables ",ylab="Cp", type='b')
points (9,hh1$cp [9], col ="red",cex=2,pch =20)
which.max(hh1$adjr2)
plot(hh1$adjr2 ,xlab="Number of Variables ",ylab="Adjusted R^2 ", type='b')
points (11,hh1$adjr2 [11], col ="red",cex=2,pch =20)
which.min(hh1$bic)

plot(hh1$bic ,xlab="Number of Variables ",ylab="BIC ", type='b')
points (3,hh1$bic [3], col ="red",cex=2,pch =20)

#for the linear regression, after using the best subset selection(including the backward selection), some important variables
#n_tokens_content    n_non_stop_words   n_non_stop_unique_tokens num_videos   self_reference_min_shares  kw_avg_avg




#summary statistics by Jiatao 
#simple table displaying counts for different type of channel 
table(new_data$channel) 
#some summary stats grouped by channel 
new_data %>% 
    group_by( channel ) %>% 
    summarise( percent = 100 * n() / nrow( new_data ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))



# merge those weekday columns into one.
Z <- new_data %>% pivot_longer(cols = weekday_is_monday:weekday_is_sunday, names_to = "weekday",values_to = 'whatever') 
X <- Z %>% filter(whatever != 0) %>% select(-whatever) 
table(X$weekday)
X %>% 
    group_by( weekday ) %>% 
    summarise( percent = 100 * n() / nrow( X ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))

table(X$weekday, X$channel)


#V <- ifelse(X$is_weekend == 0, "No","Yes")
table(X$channel, X$is_weekend)

X %>% group_by(is_weekend)%>% 
    summarise( percent = 100 * n() / nrow( X ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))











```

### Graphical summaries  

```{r}
#Scatter plot for n_tokens_content v.s. Shares.
scatter <- ggplot(data = pop.data, aes(x = n_tokens_content, y = shares))
scatter + geom_point(aes(color = is_weekend)) + 
geom_smooth(method = lm) + 
labs(title = "n_tokens_content v.s. Shares", x = "n_tokens_content", y = "shares") + 
scale_color_discrete(name = "is_weekend")





#some graphs 


g <-ggplot(X,aes(x = channel))
  g + 
    geom_bar(aes(fill = as.factor(is_weekend)),
               position = "dodge") + 
    labs(x = "channel", y = "Count", title = "channel by weekend") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    scale_fill_discrete(name = "weekend") 

  
  
  
  
  
g<-ggplot(X,aes(x = channel,
                       y = shares))
  g + 
    geom_boxplot(position = "dodge") + 
    labs(x = "y",
         title = "Boxplot for popularity with channel type ")+ 
    scale_x_discrete(name = "channel")+ 
    geom_jitter(aes(color = as.factor(weekday))) + 
    scale_y_continuous() + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    scale_color_discrete(name = "y")
  
  
  
  
  
  
  g<-ggplot(X,
          aes(x = weekday))
  g + 
    geom_bar(aes(fill = as.factor(channel)),
             position = "stack",show.legend = NA) + 
    labs(x = "weekday")+ 
    scale_fill_discrete(name = "channel") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    labs(title = "weekday by channel ")
  
 # or  
   g<-ggplot(X,
          aes(x = channel))
  g + 
    geom_bar(aes(fill = as.factor(weekday)),
             position = "stack",show.legend = NA) + 
    labs(x = "channel")+ 
    scale_fill_discrete(name = "weekday") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    labs(title = " channel by weekday ")
```

Split the data set into training and testing set. Use p = 0.7.  

```{r split.data}
# set seed
set.seed(234)

train.index <- createDataPartition(y = pop.data$shares, p = 0.7, list = F)
train <- pop.data[train.index, ] # training set
test <- pop.data[-train.index, ] # test set

#train.index<- sample(1:nrow(pop.data), size = nrow(pop.data) *0.7)
#test.index<- dplyr::setdiff(1:nrow(pop.data), train)

#train <- pop.data[train.index, ] # training set
#test <- pop.data[test.index, ] # test set
```

Linear Regression   

```{r}
# Consider all numeric variables.
lm1 <- step(lm(shares ~ . , data = train), direction = "backward")

lm2 <- step(lm(lm1$call[["formula"]], data = train), scope = . ~.^2, direction = "both")
lm2$call[["formula"]]
ctrl <- trainControl(method = "cv", number = 5)

lm.fit1 <- train(lm1$call[["formula"]], data = train, 
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)
lm.fit1$results[2:4]

lm.fit2 <- train(lm2$call[["formula"]], data = train, 
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)

# create a table to compare the results of linear regression
lm.compare <- data.frame(models= c("lm.fit1", "lm.fit2"), results = bind_rows(lm.fit1$results[2:4], lm.fit2$results[2:4]))
knitr::kable(lm.compare) 

#select the linear model with lowest RMSE. 
lm.select <- lm.compare %>% filter(results.RMSE == min(results.RMSE))

lm.select
```

Random Forest Model  

```{r}
# create dataframe for tuning parameter
rf.tGrid <- expand.grid(mtry = seq(from = 1, to = 15, by = 1))

# train the Random Forest model
rf.fit1 <- train(lm1$call[["formula"]], data = train, 
             method = "rf", trControl = ctrl, 
             preProcess = c("center", "scale"), 
             tuneGrid = rf.tGrid )

rf.fit2 <- train(lm2$call[["formula"]], data = train, 
             method = "rf", trControl = ctrl, 
             preProcess = c("center", "scale"), 
             tuneGrid = rf.tGrid )
# create a table to compare the results of linear regression
rf.compare <- data.frame(models= c("rf.fit1", "rf.fit2"), results = bind_rows(rf.fit1$results[2:4], rf.fit2$results[2:4]))
knitr::kable(rf.compare) 

#select the linear model with lowest RMSE. 
rf.select <- rf.compare %>% filter(results.RMSE == min(results.RMSE))

lm.select
```

Check model performance on test set   

```{r}
# check linear regression performance on test set 
lm.p <- confusionMatrix(data = test$shares, 
                        reference = predict(lm.fit2, newdata = test))
lm.p

# check random forest performance on test set 
rf.p <- confusionMatrix(data = test$shares, 
                        reference = predict(rf.fit2, newdata = test))
rf.p

```


```{r}
set.seed(123)
# using boosted tree 
library(gbm)
# boosted tree 
tune1 = c(25,50,70,100)
tune2 = c(1:10)
tune3 = 0.1
tune4= 10
boostTreefit <- train(shares ~ ., data = train.sub, 
                method = "gbm",
                preProcess = c("center","scale"),
              trControl = trainControl(method = "cv",number = 10),
              tuneGrid = expand.grid(n.trees = tune1,interaction.depth = tune2,shrinkage= tune3,n.minobsinnode= tune4))

plot(boostTreefit$results$n.trees, boostTreefit$results$RMSE, xlab = "n.trees",ylab = "RMSE",type = 'p',main = 'boosted')
plot(boostTreefit$results$interaction.depth, boostTreefit$results$RMSE, xlab = "subtrees",ylab = "RMSE",type = 'p',main = 'boosted')
plot(boostTreefit$results$interaction.depth, boostTreefit$results$Rsquared, xlab = "subtrees",ylab = "RMSE",type = 'p',main = 'boosted')


tune1 = 25
tune2 = 1
tune3 = 0.1
tune4= 10
boostTreefit2 <- train(shares ~ ., data = train.sub, 
                method = "gbm",
                preProcess = c("center","scale"),
              trControl = trainControl(method = "cv",number = 10),
              tuneGrid = expand.grid(n.trees = tune1,interaction.depth = tune2,shrinkage= tune3,n.minobsinnode= tune4))
pred <- predict(boostTreefit2, newdata = test.sub)

test.MSE <- mean((pred - test.sub$shares)^2)











```






