---
title: "ST558 Project 2"
author: "Aries Zhou, Jiatao Wang"
date: "10/19/2021"
---

```{r, echo = FALSE, eval = FALSE}
rmarkdown::render("./ST558_Project_2.Rmd",
                  output_format = "github_document", 
                  output_dir = "./")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.align='center', fig.path = "./")
```

## Introduction  

### Description of the data for this project 
This is a R project using the exploratory data analysis and supervised statistical learning method to analyze a data set.  
This data set is called __Online News Popularity Data Set__ and you can access the data set [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)  
There are lots of measurements/heterogeneous features of articles, including type of the data channel, number of images, number of videos, number of links, counts of words in the title/content, when it is published, summary statistics of polarity of positive/negative words and etc...  
The __main goal__ of this project is to use those features/explanatory variables to predict the popularity(number of the shares in social networks)  
Before conducting any method to fit the data with models, we want to do some exploratory data analysis (including some summary statistics and graphs) to visualize the data. And then, we will fit the data under regression setting. 
Supervised learning methods that will be used in this project include: linear regression, generalized linear model, lasso regression, random forest regression, random forest classification, boosted method, or any other method that we will find that could be applicable through our discovering of the data.  


List of packages used:  
```{r}
library(dplyr)
library(tidyr)
library(ggcorrplot)
library(vcd)
library(caret)
library(class)
library(randomForest)
library(gbm)
library(readr)
library(leaps)
library(Matrix)
library(glmnet)
library(rmarkdown)
```
## Data Cleaning 

### Data  
Read in data and combine certain numerical columns to a single column(categorical)  
```{r}
# import data
pop <- read_csv("OnlineNewsPopularity.csv")

anyNA(pop)

#convert the wide to long format (categorize data channel, and make them into one column)
new <- pop %>% pivot_longer(cols = data_channel_is_lifestyle:data_channel_is_world, names_to = "channel",values_to = 'things') 
new_data <- new %>% filter(things != 0) %>% select(-things) 
#pop.data2 <- new_data %>% subset(channel == 'data_channel_is_lifestyle') %>% select( -1:-2)

# merge those weekday columns into one.
Z <- new_data %>% pivot_longer(cols = weekday_is_monday:weekday_is_sunday, names_to = "weekday",values_to = 'whatever') 
X <- Z %>% filter(whatever != 0) %>% select(-whatever) 
pop.data2 <- X %>% subset(channel == "data_channel_is_lifestyle") %>% select( -1:-2) 
pop.data2$weekday <- as.factor(pop.data2$weekday)
str(pop.data2) 

#there are some observations that are not in the types of channel listed in the data set. 
nrow(new_data)< nrow(pop)

# subset data on data channel of interest
#pop.data <- pop %>% subset(data_channel_is_lifestyle == 1) %>% select(-starts_with("data_channel_is_"), -1:-2)
```




## Exploratory Data Analysis 
### Summarizations And Graphs  

Some tables for selected data channel of interest showing the counts and percentage grouped by channel and weekday  
```{r}
#summary statistics 
#simple table displaying counts for different type of channel (all obs)
table(X$channel) 
#some summary stats grouped by channel 
X %>% 
    group_by( channel ) %>% 
    summarise( percent = 100 * n() / nrow( new_data ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))



# using the subset data set containing weekday info in one column. 
table(pop.data2$weekday)

pop.data2 %>% 
    group_by( weekday ) %>% 
    summarise( percent = 100 * n() / nrow( X ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))

table(pop.data2$weekday, pop.data2$channel)


#V <- ifelse(X$is_weekend == 0, "No","Yes")
table(pop.data2$channel, pop.data2$is_weekend)

pop.data2 %>% group_by(is_weekend)%>% 
    summarise( percent = 100 * n() / nrow( X ),mean_shares = mean(shares), mean_images = mean(num_imgs),mean_video = mean(num_videos),mean_link = mean(num_hrefs))


```

### Graphical summaries  

```{r}
#Scatter plot for n_tokens_content v.s. Shares.
scatter <- ggplot(data = pop.data2, aes(x = n_tokens_content, y = shares))
scatter + geom_point(aes(color = as.factor(is_weekend))) + 
geom_smooth(method = lm) + 
labs(title = "n_tokens_content v.s. Shares", x = "n_tokens_content", y = "shares") + 
scale_color_discrete(name = "is_weekend")





g<-ggplot(pop.data2,aes(x = num_videos, 
                       y =shares))
  g + 
    geom_point(aes(shape = as.factor(is_weekend),
                   color = as.factor(weekday)),
               size = 2) + 
    geom_smooth(method = lm) + 
    labs(x = "Videos", 
         y = "shares",
         title = "Videos vs Shares ")+  
    scale_shape_manual(values = c(3:4))+
    scale_color_discrete(name = "weekday")+
    scale_shape_discrete(name="is.weekend")





```

#### General plots 
This is a bar plot channel by weekend(is or not)  
We can see from the plot that weekend shares far more less than the shares that is not on weekend  
```{r}
#some graphs 


g <-ggplot(X,aes(x = channel))
  g + 
    geom_bar(aes(fill = as.factor(is_weekend)),
               position = "dodge") + 
    labs(x = "channel", y = "Count", title = "channel by weekend") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    scale_fill_discrete(name = "weekend") 
```
  
  
  Boxplot for different channels  
  There are some outliers of shares in certain channels. 
```{r}  
g<-ggplot(X,aes(x = channel,
                       y = shares))
  g + 
    geom_boxplot(position = "dodge") + 
    labs(x = "y",
         title = "Boxplot for popularity with channel type ")+ 
    scale_x_discrete(name = "channel")+ 
    geom_jitter(aes(color = as.factor(weekday))) + 
    scale_y_continuous() + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    scale_color_discrete(name = "weekday")
```  
  
  
  
  This is the bar plot : channel by weekday(stacked bar)
```{r}  
  g<-ggplot(X,
          aes(x = weekday))
  g + 
    geom_bar(aes(fill = as.factor(channel)),
             position = "stack",show.legend = NA) + 
    labs(x = "weekday")+ 
    scale_fill_discrete(name = "channel") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    labs(title = "weekday by channel ")
  
 # or  
   g<-ggplot(X,
          aes(x = channel))
  g + 
    geom_bar(aes(fill = as.factor(weekday)),
             position = "stack",show.legend = NA) + 
    labs(x = "channel")+ 
    scale_fill_discrete(name = "weekday") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
    labs(title = " channel by weekday ")
```


## Modeling 

### Regression Settings
Split the data set into training and testing set. Use p = 0.7.  
```{r split.data}

pop.data2 <- X %>% subset(channel == "data_channel_is_lifestyle") %>% select( -1:-2, -49) 
# set seed
set.seed(234)

train.index <- createDataPartition(y = pop.data2$shares, p = 0.7, list = F)
train <- pop.data2[train.index, ] # training set
test <- pop.data2[-train.index, ] # test set

#train.index<- sample(1:nrow(pop.data), size = nrow(pop.data) *0.7)
#test.index<- dplyr::setdiff(1:nrow(pop.data), train)

#train <- pop.data[train.index, ] # training set
#test <- pop.data[test.index, ] # test set
```

###Linear Regression   

Before fitting any predictive models, we tried some methods that could help reduce the dimension of data.  
We randomly selected some predictors of interest and perform the best subset selection under the condition of least square linear regression.  
```{r}

# for the variable that can be used in the linear regression model. 
# try best subset selection, select number of variables using adjusted R^2, and mallow's cp, BIC,
 
final <- pop.data2 %>% select(n_tokens_content , n_non_stop_words , n_non_stop_unique_tokens , num_hrefs , num_imgs,num_keywords, num_videos , kw_avg_max , kw_min_avg , kw_max_avg , kw_avg_avg , self_reference_min_shares , self_reference_avg_sharess ,global_rate_positive_words,rate_positive_words, abs_title_subjectivity,abs_title_sentiment_polarity,shares)


train.index.sub <- createDataPartition(y = final$shares, p = 0.7, list = F)
train.sub <- final[train.index.sub, ] # training set
test.sub <- final[-train.index.sub, ] # test set


regression1 <- regsubsets(shares ~., data = train.sub,nvmax=17)
hh1<-summary(regression1)


par(mfrow=c(2,2))
which.min(hh1$cp)
plot(hh1$cp ,xlab="Number of Variables ",ylab="Cp", type='b')
points (9,hh1$cp [9], col ="red",cex=2,pch =20)
which.max(hh1$adjr2)
plot(hh1$adjr2 ,xlab="Number of Variables ",ylab="Adjusted R^2 ", type='b')
points (13,hh1$adjr2 [13], col ="red",cex=2,pch =20)
which.min(hh1$bic)
plot(hh1$bic,xlab="Number of Variables ",ylab="BIC ", type='b')
points (3,hh1$bic [3], col ="red",cex=2,pch =20)

#for the linear regression, after using the best subset selection, some important variables are 
#n_tokens_content num_videos   n_non_stop_words   n_non_stop_unique_tokens    self_reference_min_shares  kw_avg_avg  abs_title_subjectivity  kw_max_avg
```



Using all predictors, predictors chosen from backward selection, and best subset selection to fit the linear model.  
```{r, eval=FALSE}
# Consider all numeric variables.
lm1 <- step(lm(shares ~ . , data = train), direction = "backward")

lm2 <- step(lm(lm1$call[["formula"]], data = train), scope = . ~.^2, direction = "both")
lm2$call[["formula"]]

ctrl <- trainControl(method = "cv", number = 5)

lm.fit1 <- train(lm1$call[["formula"]], data = train, 
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)
lm.fit1$results[2:4]

lm.fit2 <- train(lm2$call[["formula"]], data = train, 
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)

#check the model selected by the best subset selection 
lm.fit3 <- train(shares ~ n_tokens_content +num_videos+n_non_stop_words+n_non_stop_unique_tokens + self_reference_min_shares +kw_avg_avg + abs_title_subjectivity + kw_max_avg,
                 data = train,
                 method = "lm", preProcess =c("center", "scale"), 
                 trControl = ctrl)

# create a table to compare the results of linear regression
lm.compare <- data.frame(models= c("lm.fit1", "lm.fit2","lm.fit3"), results = bind_rows(lm.fit1$results[2:4], lm.fit2$results[2:4], lm.fit3$results[2:4]))
knitr::kable(lm.compare) 

#select the linear model with lowest RMSE. 
lm.select <- lm.compare %>% filter(results.RMSE == min(results.RMSE))

lm.select
```

```{r}
predictions <- lm(as.symbol(lm.select[[1]]), data = train) %>% predict(test)

knitr::kable(data.frame( R2 = R2(predictions, test$shares),
                         RMSE = RMSE(predictions, test$shares),
                         MAE = MAE(predictions, test$shares)))
sum.results <- data.frame(Data = c("Train", "Test"), 
                          Rsquared = c(lm.select[[3]], R2(predictions, test$shares)),
                          RMSE = c(lm.select[[2]], RMSE(predictions, test$shares)),
                          MAE = c(lm.select[[4]], MAE(predictions, test$shares)))

knitr::kable(sum.results)
```

### Lasso Regression 
Since lasso perform the variable selection, we tried to use Lasso Regression(adding tuning parameter/ penalty) 
```{r}
library(Matrix)
library(glmnet)
# using all predictors (52 predictors)
cv.out.full <- cv.glmnet(as.matrix(train), train$shares, alpha=1)
#MSE versus the log(lambda)
plot(cv.out.full)
best.lambda.full <- cv.out.full$lambda.min

#fitting the lasso regression 
lasso.fit.full <- glmnet(train[,-53] ,train$shares, alpha = 1, lambda = best.lambda.full)
lasso.coef.full <- predict(lasso.fit.full, type = "coefficients")
print(lasso.coef.full)

lasso.pred.full <- predict(lasso.fit.full, newx= as.matrix(test[,-15]))
lasso.full.MSE <- mean((lasso.pred.full - test$shares)^2)

sqrt(lasso.full.MSE)







#using selected predictors (12 predictors) 
#use k-fold cv to select best lambda for the lasso regression 
cv.out <- cv.glmnet(as.matrix(train.sub), train.sub$shares, alpha=1)
#MSE versus the log(lambda)
plot(cv.out)
best.lambda <- cv.out$lambda.min

#fitting the lasso regression 
lasso.fit <- glmnet(train.sub[,-13] ,train.sub$shares, alpha = 1, lambda = best.lambda)
lasso.coef <- predict(lasso.fit, type = "coefficients")
print(lasso.coef)

lasso.pred <- predict(lasso.fit, newx= as.matrix(test.sub[,-13]))
lasso.MSE <- mean((lasso.pred - test.sub$shares)^2)
sqrt(lasso.MSE)







```



###Random Forest Regression  

```{r, eval=FALSE}
# create dataframe for tuning parameter
rf.tGrid <- expand.grid(mtry = seq(from = 1, to = 15, by = 1))

# train the Random Forest model
rf.fit1 <- train(lm1$call[["formula"]], data = train, 
             method = "rf", trControl = ctrl, 
             preProcess = c("center", "scale"), 
             tuneGrid = rf.tGrid )

rf.fit2 <- train(lm2$call[["formula"]], data = train, 
             method = "rf", trControl = ctrl, 
             preProcess = c("center", "scale"), 
             tuneGrid = rf.tGrid )

# create a table to compare the results of linear regression
rf.compare <- data.frame(models= c("rf.fit1", "rf.fit2"), results = bind_rows(rf.fit1$results[2:4], rf.fit2$results[2:4]))
knitr::kable(rf.compare) 

#select the linear model with lowest RMSE. 
rf.select <- rf.compare %>% filter(results.RMSE == min(results.RMSE))

lm.select
```

### Boosting model.(Stochastic Gradient Boosting) 
```{r}

# using boosted method 


#final <- pop.data %>% select(n_tokens_content , n_non_stop_words , n_non_stop_unique_tokens , num_hrefs , num_videos , kw_avg_max , kw_min_avg , kw_max_avg , kw_avg_avg , self_reference_min_shares , self_reference_avg_sharess , abs_title_subjectivity,shares)


#train.index <- createDataPartition(y = final$shares, p = 0.7, list = F)
#train.sub <- final[train.index, ] # training set
#test.sub <- final[-train.index, ] # test set

library(gbm)
# boosted tree 
tune1 = c(25,50,70,100)
tune2 = c(1:10)
tune3 = 0.01
tune4= 10
boostTreefit <- train(shares ~ ., data = train.sub, 
                method = "gbm",
                preProcess = c("center","scale"),
              trControl = trainControl(method = "cv",number = 10),
              tuneGrid = expand.grid(n.trees = tune1,interaction.depth = tune2,shrinkage= tune3,n.minobsinnode= tune4))
par(mfrow=c(2,2))
plot(boostTreefit$results$n.trees, boostTreefit$results$RMSE, xlab = "n.trees",ylab = "RMSE",type = 'p',main = 'boosted')
plot(boostTreefit$results$interaction.depth, boostTreefit$results$RMSE, xlab = "subtrees",ylab = "RMSE",type = 'p',main = 'boosted')
plot(boostTreefit$results$interaction.depth, boostTreefit$results$Rsquared, xlab = "subtrees",ylab = "R^2",type = 'p',main = 'boosted')


tune1 = 50
tune2 = 10
tune3 = 0.01
tune4= 10
boostTreefit2 <- train(shares ~ ., data = train.sub, 
                method = "gbm",
                preProcess = c("center","scale"),
              trControl = trainControl(method = "cv",number = 10),
              tuneGrid = expand.grid(n.trees = tune1,interaction.depth = tune2,shrinkage= tune3,n.minobsinnode= tune4))
pred.boost <- predict(boostTreefit2, newdata = test.sub)

test.MSE.boost <- mean((pred.boost - test.sub$shares)^2)
sqrt(test.MSE.boost)




```
### Discussion and Model Selection 
lm.fit1 is using backward selection to select varaibles of most interest.  
lm.fit2 is adding the interaction terms to the model fitting  
im.fit3 is chosen by the best subset selection  
lasso.full.fit is using the lasso regression to fit the model(it also perform varaible selection)  
random forest.fit   
boost.fit is using cross validation to select appropriate tuning parameter for the boosted model and use it for prediction.  
This is a simple table containing these methods and the Root Mean Square Error for each model fitting.  
Lasso perform best in this situation.  

```{r, eval=FALSE}
all.compare <- data.frame(models= c("lm.fit1", "lm.fit2","lm.fit3","lasso.full.fit","lasso.12.fit","randomForest.fit","boost.fit"), results = rbind(lm.fit1$results[2], lm.fit2$results[2], lm.fit3$results[2],sqrt(lasso.full.MSE),sqrt(lasso.MSE),",,,",sqrt(test.MSE.boost)))
knitr::kable(all.compare) 








```


### Automation of data_channel is***

```{r, eval = FALSE}
channelIDs <- unique(X$channel)
output_file <- paste0(channelIDs,".html")
params = lapply(channelIDs, FUN = function(x){list(channel= x)})
reports <- tibble(output_file, params)
library(rmarkdown)

apply(reports, MARGIN = 1,
      FUN = function(x){
        render(input = "./testing.Rmd",output_file = x[[1]], params = x[[2]])
      })
```








```